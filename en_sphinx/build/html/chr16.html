<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Chapter 16 Supervised learning methods &mdash; biopython_en 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="biopython_en 0.1 documentation" href="index.html" />
    <link rel="next" title="Chapter 17 Graphics including GenomeDiagram" href="chr17.html" />
    <link rel="prev" title="Chapter 15 Cluster analysis" href="chr15.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="chr17.html" title="Chapter 17 Graphics including GenomeDiagram"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="chr15.html" title="Chapter 15 Cluster analysis"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">biopython_en 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="chapter-16-supervised-learning-methods">
<h1>Chapter 16  Supervised learning methods<a class="headerlink" href="#chapter-16-supervised-learning-methods" title="Permalink to this headline">¶</a></h1>
<p>Note the supervised learning methods described in this chapter all
require Numerical Python (numpy) to be installed.</p>
<div class="section" id="the-logistic-regression-model">
<h2>16.1  The Logistic Regression Model<a class="headerlink" href="#the-logistic-regression-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="background-and-purpose">
<h3>16.1.1  Background and Purpose<a class="headerlink" href="#background-and-purpose" title="Permalink to this headline">¶</a></h3>
<p>Logistic regression is a supervised learning approach that attempts to
distinguish <em>K</em> classes from each other using a weighted sum of some
predictor variables <em>x</em><sub>*i*</sub>. The logistic regression model is
used to calculate the weights β<sub>*i*</sub> of the predictor variables.
In Biopython, the logistic regression model is currently implemented for
two classes only (<em>K</em> = 2); the number of predictor variables has no
predefined limit.</p>
<p>As an example, let’s try to predict the operon structure in bacteria. An
operon is a set of adjacent genes on the same strand of DNA that are
transcribed into a single mRNA molecule. Translation of the single mRNA
molecule then yields the individual proteins. For <em>Bacillus subtilis</em>,
whose data we will be using, the average number of genes in an operon is
about 2.4.</p>
<p>As a first step in understanding gene regulation in bacteria, we need to
know the operon structure. For about 10% of the genes in <em>Bacillus
subtilis</em>, the operon structure is known from experiments. A supervised
learning method can be used to predict the operon structure for the
remaining 90% of the genes.</p>
<p>For such a supervised learning approach, we need to choose some
predictor variables <em>x</em><sub>*i*</sub> that can be measured easily and are
somehow related to the operon structure. One predictor variable might be
the distance in base pairs between genes. Adjacent genes belonging to
the same operon tend to be separated by a relatively short distance,
whereas adjacent genes in different operons tend to have a larger space
between them to allow for promoter and terminator sequences. Another
predictor variable is based on gene expression measurements. By
definition, genes belonging to the same operon have equal gene
expression profiles, while genes in different operons are expected to
have different expression profiles. In practice, the measured expression
profiles of genes in the same operon are not quite identical due to the
presence of measurement errors. To assess the similarity in the gene
expression profiles, we assume that the measurement errors follow a
normal distribution and calculate the corresponding log-likelihood
score.</p>
<p>We now have two predictor variables that we can use to predict if two
adjacent genes on the same strand of DNA belong to the same operon:</p>
<ul class="simple">
<li><em>x</em><sub>1</sub>: the number of base pairs between them;</li>
<li><em>x</em><sub>2</sub>: their similarity in expression profile.</li>
</ul>
<p>In a logistic regression model, we use a weighted sum of these two
predictors to calculate a joint score <em>S</em>:</p>
<div class="math">
\[\begin{equation}
S = \beta_0 + \beta_1 x_1 + \beta_2 x_2.
\end{equation}\]</div>
<p>The logistic regression model gives us appropriate values for the
parameters β<sub>0</sub>, β<sub>1</sub>, β<sub>2</sub> using two sets of
example genes:</p>
<ul class="simple">
<li>OP: Adjacent genes, on the same strand of DNA, known to belong to the
same operon;</li>
<li>NOP: Adjacent genes, on the same strand of DNA, known to belong to
different operons.</li>
</ul>
<p>In the logistic regression model, the probability of belonging to a
class depends on the score via the logistic function. For the two
classes OP and NOP, we can write this as</p>
<div class="math">
\[\begin{split}\begin{eqnarray}
\Pr(\mathrm{OP}|x_1, x_2) &amp; = &amp; \frac{\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:OP} \\
\Pr(\mathrm{NOP}|x_1, x_2) &amp; = &amp; \frac{1}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:NOP}
\end{eqnarray}\end{split}\]</div>
<p>Using a set of gene pairs for which it is known whether they belong to
the same operon (class OP) or to different operons (class NOP), we can
calculate the weights β<sub>0</sub>, β<sub>1</sub>, β<sub>2</sub> by
maximizing the log-likelihood corresponding to the probability functions
(<a class="reference external" href="#eq:OP">16.2</a>) and (<a class="reference external" href="#eq:NOP">16.3</a>).</p>
</div>
<div class="section" id="training-the-logistic-regression-model">
<h3>16.1.2  Training the logistic regression model<a class="headerlink" href="#training-the-logistic-regression-model" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Table 16.1: Adjacent gene pairs known to belong to the same operon (class OP) or to different operons (class NOP). Intergene distances are negative if the two genes overlap.</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="35%" />
<col width="38%" />
<col width="8%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Gene pair</td>
<td>Intergene distance (<em>x</em><sub>1</sub>)</td>
<td>Gene expression score (<em>x</em><sub>2</sub>)</td>
<td>Class</td>
</tr>
<tr class="row-even"><td><em>cotJA</em> — <em>cotJB</em></td>
<td>-53</td>
<td>-200.78</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>yesK</em> — <em>yesL</em></td>
<td>117</td>
<td>-267.14</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>lplA</em> — <em>lplB</em></td>
<td>57</td>
<td>-163.47</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>lplB</em> — <em>lplC</em></td>
<td>16</td>
<td>-190.30</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>lplC</em> — <em>lplD</em></td>
<td>11</td>
<td>-220.94</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>lplD</em> — <em>yetF</em></td>
<td>85</td>
<td>-193.94</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>yfmT</em> — <em>yfmS</em></td>
<td>16</td>
<td>-182.71</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>yfmF</em> — <em>yfmE</em></td>
<td>15</td>
<td>-180.41</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>citS</em> — <em>citT</em></td>
<td>-26</td>
<td>-181.73</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>citM</em> — <em>yflN</em></td>
<td>58</td>
<td>-259.87</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>yfiI</em> — <em>yfiJ</em></td>
<td>126</td>
<td>-414.53</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>lipB</em> — <em>yfiQ</em></td>
<td>191</td>
<td>-249.57</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>yfiU</em> — <em>yfiV</em></td>
<td>113</td>
<td>-265.28</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>yfhH</em> — <em>yfhI</em></td>
<td>145</td>
<td>-312.99</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>cotY</em> — <em>cotX</em></td>
<td>154</td>
<td>-213.83</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>yjoB</em> — <em>rapA</em></td>
<td>147</td>
<td>-380.85</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>ptsI</em> — <em>splA</em></td>
<td>93</td>
<td>-291.13</td>
<td>NOP</td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>Table <a class="reference external" href="#table:training">16.1</a> lists some of the <em>Bacillus subtilis</em>
gene pairs for which the operon structure is known. Let’s calculate the
logistic regression model from these data:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">53</span><span class="p">,</span> <span class="o">-</span><span class="mf">200.78</span><span class="p">],</span>
<span class="go">          [117, -267.14],</span>
<span class="go">          [57, -163.47],</span>
<span class="go">          [16, -190.30],</span>
<span class="go">          [11, -220.94],</span>
<span class="go">          [85, -193.94],</span>
<span class="go">          [16, -182.71],</span>
<span class="go">          [15, -180.41],</span>
<span class="go">          [-26, -181.73],</span>
<span class="go">          [58, -259.87],</span>
<span class="go">          [126, -414.53],</span>
<span class="go">          [191, -249.57],</span>
<span class="go">          [113, -265.28],</span>
<span class="go">          [145, -312.99],</span>
<span class="go">          [154, -213.83],</span>
<span class="go">          [147, -380.85],</span>
<span class="go">          [93, -291.13]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <tt class="docutils literal"><span class="pre">xs</span></tt> and <tt class="docutils literal"><span class="pre">ys</span></tt> are the training data: <tt class="docutils literal"><span class="pre">xs</span></tt> contains the
predictor variables for each gene pair, and <tt class="docutils literal"><span class="pre">ys</span></tt> specifies if the gene
pair belongs to the same operon (<tt class="docutils literal"><span class="pre">1</span></tt>, class OP) or different operons
(<tt class="docutils literal"><span class="pre">0</span></tt>, class NOP). The resulting logistic regression model is stored in
<tt class="docutils literal"><span class="pre">model</span></tt>, which contains the weights β<sub>0</sub>, β<sub>1</sub>, and
β<sub>2</sub>:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">beta</span>
<span class="go">[8.9830290157144681, -0.035968960444850887, 0.02181395662983519]</span>
</pre></div>
</div>
<p>Note that β<sub>1</sub> is negative, as gene pairs with a shorter
intergene distance have a higher probability of belonging to the same
operon (class OP). On the other hand, β<sub>2</sub> is positive, as gene
pairs belonging to the same operon typically have a higher similarity
score of their gene expression profiles. The parameter β<sub>0</sub> is
positive due to the higher prevalence of operon gene pairs than
non-operon gene pairs in the training data.</p>
<p>The function <tt class="docutils literal"><span class="pre">train</span></tt> has two optional arguments: <tt class="docutils literal"><span class="pre">update_fn</span></tt> and
<tt class="docutils literal"><span class="pre">typecode</span></tt>. The <tt class="docutils literal"><span class="pre">update_fn</span></tt> can be used to specify a callback
function, taking as arguments the iteration number and the
log-likelihood. With the callback function, we can for example track the
progress of the model calculation (which uses a Newton-Raphson iteration
to maximize the log-likelihood function of the logistic regression
model):</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">show_progress</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
<span class="go">        print &quot;Iteration:&quot;, iteration, &quot;Log-likelihood function:&quot;, loglikelihood</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">update_fn</span><span class="o">=</span><span class="n">show_progress</span><span class="p">)</span>
<span class="go">Iteration: 0 Log-likelihood function: -11.7835020695</span>
<span class="go">Iteration: 1 Log-likelihood function: -7.15886767672</span>
<span class="go">Iteration: 2 Log-likelihood function: -5.76877209868</span>
<span class="go">Iteration: 3 Log-likelihood function: -5.11362294338</span>
<span class="go">Iteration: 4 Log-likelihood function: -4.74870642433</span>
<span class="go">Iteration: 5 Log-likelihood function: -4.50026077146</span>
<span class="go">Iteration: 6 Log-likelihood function: -4.31127773737</span>
<span class="go">Iteration: 7 Log-likelihood function: -4.16015043396</span>
<span class="go">Iteration: 8 Log-likelihood function: -4.03561719785</span>
<span class="go">Iteration: 9 Log-likelihood function: -3.93073282192</span>
<span class="go">Iteration: 10 Log-likelihood function: -3.84087660929</span>
<span class="go">Iteration: 11 Log-likelihood function: -3.76282560605</span>
<span class="go">Iteration: 12 Log-likelihood function: -3.69425027154</span>
<span class="go">Iteration: 13 Log-likelihood function: -3.6334178602</span>
<span class="go">Iteration: 14 Log-likelihood function: -3.57900855837</span>
<span class="go">Iteration: 15 Log-likelihood function: -3.52999671386</span>
<span class="go">Iteration: 16 Log-likelihood function: -3.48557145163</span>
<span class="go">Iteration: 17 Log-likelihood function: -3.44508206139</span>
<span class="go">Iteration: 18 Log-likelihood function: -3.40799948447</span>
<span class="go">Iteration: 19 Log-likelihood function: -3.3738885624</span>
<span class="go">Iteration: 20 Log-likelihood function: -3.3423876581</span>
<span class="go">Iteration: 21 Log-likelihood function: -3.31319343769</span>
<span class="go">Iteration: 22 Log-likelihood function: -3.2860493346</span>
<span class="go">Iteration: 23 Log-likelihood function: -3.2607366863</span>
<span class="go">Iteration: 24 Log-likelihood function: -3.23706784091</span>
<span class="go">Iteration: 25 Log-likelihood function: -3.21488073614</span>
<span class="go">Iteration: 26 Log-likelihood function: -3.19403459259</span>
<span class="go">Iteration: 27 Log-likelihood function: -3.17440646052</span>
<span class="go">Iteration: 28 Log-likelihood function: -3.15588842703</span>
<span class="go">Iteration: 29 Log-likelihood function: -3.13838533947</span>
<span class="go">Iteration: 30 Log-likelihood function: -3.12181293595</span>
<span class="go">Iteration: 31 Log-likelihood function: -3.10609629966</span>
<span class="go">Iteration: 32 Log-likelihood function: -3.09116857282</span>
<span class="go">Iteration: 33 Log-likelihood function: -3.07696988017</span>
<span class="go">Iteration: 34 Log-likelihood function: -3.06344642288</span>
<span class="go">Iteration: 35 Log-likelihood function: -3.05054971191</span>
<span class="go">Iteration: 36 Log-likelihood function: -3.03823591619</span>
<span class="go">Iteration: 37 Log-likelihood function: -3.02646530573</span>
<span class="go">Iteration: 38 Log-likelihood function: -3.01520177394</span>
<span class="go">Iteration: 39 Log-likelihood function: -3.00441242601</span>
<span class="go">Iteration: 40 Log-likelihood function: -2.99406722296</span>
<span class="go">Iteration: 41 Log-likelihood function: -2.98413867259</span>
</pre></div>
</div>
<p>The iteration stops once the increase in the log-likelihood function is
less than 0.01. If no convergence is reached after 500 iterations, the
<tt class="docutils literal"><span class="pre">train</span></tt> function returns with an <tt class="docutils literal"><span class="pre">AssertionError</span></tt>.</p>
<p>The optional keyword <tt class="docutils literal"><span class="pre">typecode</span></tt> can almost always be ignored. This
keyword allows the user to choose the type of Numeric matrix to use. In
particular, to avoid memory problems for very large problems, it may be
necessary to use single-precision floats (Float8, Float16, etc.) rather
than double, which is used by default.</p>
</div>
<div class="section" id="using-the-logistic-regression-model-for-classification">
<h3>16.1.3  Using the logistic regression model for classification<a class="headerlink" href="#using-the-logistic-regression-model-for-classification" title="Permalink to this headline">¶</a></h3>
<p>Classification is performed by calling the <tt class="docutils literal"><span class="pre">classify</span></tt> function. Given
a logistic regression model and the values for <em>x</em><sub>1</sub> and
<em>x</em><sub>2</sub> (e.g. for a gene pair of unknown operon structure), the
<tt class="docutils literal"><span class="pre">classify</span></tt> function returns <tt class="docutils literal"><span class="pre">1</span></tt> or <tt class="docutils literal"><span class="pre">0</span></tt>, corresponding to class OP
and class NOP, respectively. For example, let’s consider the gene pairs
<em>yxcE</em>, <em>yxcD</em> and <em>yxiB</em>, <em>yxiA</em>:</p>
<hr class="docutils" />
<table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Table 16.2: Adjacent gene pairs of unknown operon status.</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="20%" />
<col width="38%" />
<col width="41%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Gene pair</td>
<td>Intergene distance <em>x</em><sub>1</sub></td>
<td>Gene expression score <em>x</em><sub>2</sub></td>
</tr>
<tr class="row-even"><td><em>yxcE</em> — <em>yxcD</em></td>
<td>6</td>
<td>-173.143442352</td>
</tr>
<tr class="row-odd"><td><em>yxiB</em> — <em>yxiA</em></td>
<td>309</td>
<td>-271.005880394</td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>The logistic regression model classifies <em>yxcE</em>, <em>yxcD</em> as belonging to
the same operon (class OP), while <em>yxiB</em>, <em>yxiA</em> are predicted to belong
to different operons:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
<span class="go">yxcE, yxcD: 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxiB, yxiA:&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
<span class="go">yxiB, yxiA: 0</span>
</pre></div>
</div>
<p>(which, by the way, agrees with the biological literature).</p>
<p>To find out how confident we can be in these predictions, we can call
the <tt class="docutils literal"><span class="pre">calculate</span></tt> function to obtain the probabilities (equations
(<a class="reference external" href="#eq:OP">16.2</a>) and (<a class="reference external" href="#eq:NOP">16.3</a>)) for class OP and NOP. For
<em>yxcE</em>, <em>yxcD</em> we find</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: probability =&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s">&quot;class NOP: probability =&quot;</span><span class="p">,</span> <span class="n">q</span>
<span class="go">class OP: probability = 0.993242163503 class NOP: probability = 0.00675783649744</span>
</pre></div>
</div>
<p>and for <em>yxiB</em>, <em>yxiA</em></p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: probability =&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s">&quot;class NOP: probability =&quot;</span><span class="p">,</span> <span class="n">q</span>
<span class="go">class OP: probability = 0.000321211251817 class NOP: probability = 0.999678788748</span>
</pre></div>
</div>
<p>To get some idea of the prediction accuracy of the logistic regression
model, we can apply it to the training data:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, LogisticRegression.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>showing that the prediction is correct for all but one of the gene
pairs. A more reliable estimate of the prediction accuracy can be found
from a leave-one-out analysis, in which the model is recalculated from
the training data after removing the gene to be predicted:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        model = LogisticRegression.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:])</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, LogisticRegression.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>The leave-one-out analysis shows that the prediction of the logistic
regression model is incorrect for only two of the gene pairs, which
corresponds to a prediction accuracy of 88%.</p>
</div>
<div class="section" id="logistic-regression-linear-discriminant-analysis-and-support-vector-machines">
<h3>16.1.4  Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines<a class="headerlink" href="#logistic-regression-linear-discriminant-analysis-and-support-vector-machines" title="Permalink to this headline">¶</a></h3>
<p>The logistic regression model is similar to linear discriminant
analysis. In linear discriminant analysis, the class probabilities also
follow equations (<a class="reference external" href="#eq:OP">16.2</a>) and (<a class="reference external" href="#eq:NOP">16.3</a>). However,
instead of estimating the coefficients β directly, we first fit a normal
distribution to the predictor variables <em>x</em>. The coefficients β are then
calculated from the means and covariances of the normal distribution. If
the distribution of <em>x</em> is indeed normal, then we expect linear
discriminant analysis to perform better than the logistic regression
model. The logistic regression model, on the other hand, is more robust
to deviations from normality.</p>
<p>Another similar approach is a support vector machine with a linear
kernel. Such an SVM also uses a linear combination of the predictors,
but estimates the coefficients β from the predictor variables <em>x</em> near
the boundary region between the classes. If the logistic regression
model (equations (<a class="reference external" href="#eq:OP">16.2</a>) and (<a class="reference external" href="#eq:NOP">16.3</a>)) is a good
description for <em>x</em> away from the boundary region, we expect the
logistic regression model to perform better than an SVM with a linear
kernel, as it relies on more data. If not, an SVM with a linear kernel
may perform better.</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman: <em>The Elements of
Statistical Learning. Data Mining, Inference, and Prediction</em>. Springer
Series in Statistics, 2001. Chapter 4.4.</p>
</div>
</div>
<div class="section" id="k-nearest-neighbors">
<h2>16.2  <em>k</em>-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>16.2.1  Background and purpose<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The <em>k</em>-nearest neighbors method is a supervised learning approach that
does not need to fit a model to the data. Instead, data points are
classified based on the categories of the <em>k</em> nearest neighbors in the
training data set.</p>
<p>In Biopython, the <em>k</em>-nearest neighbors method is available in
<tt class="docutils literal"><span class="pre">Bio.kNN</span></tt>. To illustrate the use of the <em>k</em>-nearest neighbor method in
Biopython, we will use the same operon data set as in section
<a class="reference external" href="#sec:LogisticRegression">16.1</a>.</p>
</div>
<div class="section" id="initializing-a-k-nearest-neighbors-model">
<h3>16.2.2  Initializing a <em>k</em>-nearest neighbors model<a class="headerlink" href="#initializing-a-k-nearest-neighbors-model" title="Permalink to this headline">¶</a></h3>
<p>Using the data in Table <a class="reference external" href="#table:training">16.1</a>, we create and
initialize a <em>k</em>-nearest neighbors model as follows:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">kNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
<p>where <tt class="docutils literal"><span class="pre">xs</span></tt> and <tt class="docutils literal"><span class="pre">ys</span></tt> are the same as in Section
<a class="reference external" href="#subsec:LogisticRegressionTraining">16.1.2</a>. Here, <tt class="docutils literal"><span class="pre">k</span></tt> is the
number of neighbors <em>k</em> that will be considered for the classification.
For classification into two classes, choosing an odd number for <em>k</em> lets
you avoid tied votes. The function name <tt class="docutils literal"><span class="pre">train</span></tt> is a bit of a
misnomer, since no model training is done: this function simply stores
<tt class="docutils literal"><span class="pre">xs</span></tt>, <tt class="docutils literal"><span class="pre">ys</span></tt>, and <tt class="docutils literal"><span class="pre">k</span></tt> in <tt class="docutils literal"><span class="pre">model</span></tt>.</p>
</div>
<div class="section" id="using-a-k-nearest-neighbors-model-for-classification">
<h3>16.2.3  Using a <em>k</em>-nearest neighbors model for classification<a class="headerlink" href="#using-a-k-nearest-neighbors-model-for-classification" title="Permalink to this headline">¶</a></h3>
<p>To classify new data using the <em>k</em>-nearest neighbors model, we use the
<tt class="docutils literal"><span class="pre">classify</span></tt> function. This function takes a data point
(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>) and finds the <em>k</em>-nearest neighbors in
the training data set <tt class="docutils literal"><span class="pre">xs</span></tt>. The data point (<em>x</em><sub>1</sub>,
<em>x</em><sub>2</sub>) is then classified based on which category (<tt class="docutils literal"><span class="pre">ys</span></tt>)
occurs most among the <em>k</em> neighbors.</p>
<p>For the example of the gene pairs <em>yxcE</em>, <em>yxcD</em> and <em>yxiB</em>, <em>yxiA</em>, we
find:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxiB, yxiA:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">yxiB, yxiA: 0</span>
</pre></div>
</div>
<p>In agreement with the logistic regression model, <em>yxcE</em>, <em>yxcD</em> are
classified as belonging to the same operon (class OP), while <em>yxiB</em>,
<em>yxiA</em> are predicted to belong to different operons.</p>
<p>The <tt class="docutils literal"><span class="pre">classify</span></tt> function lets us specify both a distance function and a
weight function as optional arguments. The distance function affects
which <em>k</em> neighbors are chosen as the nearest neighbors, as these are
defined as the neighbors with the smallest distance to the query point
(<em>x</em>, <em>y</em>). By default, the Euclidean distance is used. Instead, we
could for example use the city-block (Manhattan) distance:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">cityblock</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="n">distance</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">distance_fn</span> <span class="o">=</span> <span class="n">cityblock</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
</pre></div>
</div>
<p>The weight function can be used for weighted voting. For example, we may
want to give closer neighbors a higher weight than neighbors that are
further away:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight_fn</span> <span class="o">=</span> <span class="n">weight</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
</pre></div>
</div>
<p>By default, all neighbors are given an equal weight.</p>
<p>To find out how confident we can be in these predictions, we can call
the <tt class="docutils literal"><span class="pre">calculate</span></tt> function, which will calculate the total weight
assigned to the classes OP and NOP. For the default weighting scheme,
this reduces to the number of neighbors in each category. For <em>yxcE</em>,
<em>yxcD</em>, we find</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">&quot;class NOP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">class OP: weight = 0.0 class NOP: weight = 3.0</span>
</pre></div>
</div>
<p>which means that all three neighbors of <tt class="docutils literal"><span class="pre">x1</span></tt>, <tt class="docutils literal"><span class="pre">x2</span></tt> are in the NOP
class. As another example, for <em>yesK</em>, <em>yesL</em> we find</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">117</span><span class="p">,</span> <span class="o">-</span><span class="mf">267.14</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">&quot;class NOP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">class OP: weight = 2.0 class NOP: weight = 1.0</span>
</pre></div>
</div>
<p>which means that two neighbors are operon pairs and one neighbor is a
non-operon pair.</p>
<p>To get some idea of the prediction accuracy of the <em>k</em>-nearest neighbors
approach, we can apply it to the training data:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, kNN.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>showing that the prediction is correct for all but two of the gene
pairs. A more reliable estimate of the prediction accuracy can be found
from a leave-one-out analysis, in which the model is recalculated from
the training data after removing the gene to be predicted:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        model = kNN.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:])</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, kNN.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
</pre></div>
</div>
<p>The leave-one-out analysis shows that <em>k</em>-nearest neighbors model is
correct for 13 out of 17 gene pairs, which corresponds to a prediction
accuracy of 76%.</p>
</div>
</div>
<div class="section" id="naive-bayes">
<h2>16.3  Naïve Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h2>
<p>This section will describe the <tt class="docutils literal"><span class="pre">Bio.NaiveBayes</span></tt> module.</p>
</div>
<div class="section" id="maximum-entropy">
<h2>16.4  Maximum Entropy<a class="headerlink" href="#maximum-entropy" title="Permalink to this headline">¶</a></h2>
<p>This section will describe the <tt class="docutils literal"><span class="pre">Bio.MaximumEntropy</span></tt> module.</p>
</div>
<div class="section" id="markov-models">
<h2>16.5  Markov Models<a class="headerlink" href="#markov-models" title="Permalink to this headline">¶</a></h2>
<p>This section will describe the <tt class="docutils literal"><span class="pre">Bio.MarkovModel</span></tt> and/or
<tt class="docutils literal"><span class="pre">Bio.HMM.MarkovModel</span></tt> modules.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Chapter 16  Supervised learning methods</a><ul>
<li><a class="reference internal" href="#the-logistic-regression-model">16.1  The Logistic Regression Model</a><ul>
<li><a class="reference internal" href="#background-and-purpose">16.1.1  Background and Purpose</a></li>
<li><a class="reference internal" href="#training-the-logistic-regression-model">16.1.2  Training the logistic regression model</a></li>
<li><a class="reference internal" href="#using-the-logistic-regression-model-for-classification">16.1.3  Using the logistic regression model for classification</a></li>
<li><a class="reference internal" href="#logistic-regression-linear-discriminant-analysis-and-support-vector-machines">16.1.4  Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines</a></li>
</ul>
</li>
<li><a class="reference internal" href="#k-nearest-neighbors">16.2  <em>k</em>-Nearest Neighbors</a><ul>
<li><a class="reference internal" href="#id1">16.2.1  Background and purpose</a></li>
<li><a class="reference internal" href="#initializing-a-k-nearest-neighbors-model">16.2.2  Initializing a <em>k</em>-nearest neighbors model</a></li>
<li><a class="reference internal" href="#using-a-k-nearest-neighbors-model-for-classification">16.2.3  Using a <em>k</em>-nearest neighbors model for classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#naive-bayes">16.3  Naïve Bayes</a></li>
<li><a class="reference internal" href="#maximum-entropy">16.4  Maximum Entropy</a></li>
<li><a class="reference internal" href="#markov-models">16.5  Markov Models</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="chr15.html"
                        title="previous chapter">Chapter 15  Cluster analysis</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="chr17.html"
                        title="next chapter">Chapter 17  Graphics including GenomeDiagram</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/chr16.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="chr17.html" title="Chapter 17 Graphics including GenomeDiagram"
             >next</a> |</li>
        <li class="right" >
          <a href="chr15.html" title="Chapter 15 Cluster analysis"
             >previous</a> |</li>
        <li><a href="index.html">biopython_en 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, biopythoners.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.
    </div>
  </body>
</html>